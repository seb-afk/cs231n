{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"cs231n/datasets/coco_captioning/\"\n",
    "images = pd.read_hdf(fp+\"train2014_images.txt\")\n",
    "images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs231n.rnn_layers import rnn_step_forward\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape\n",
    "# def rnn_forward(x, h0, Wx, Wh, b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = list()\n",
    "caches_timestep = list()\n",
    "\n",
    "current_h = h0\n",
    "for t_i in range(T):\n",
    "    next_h, next_cache = rnn_step_forward(x[:,t_i,:], current_h, Wx, Wh, b)\n",
    "    hidden_states.append(next_h)\n",
    "    caches_timestep.append(next_cache)\n",
    "    current_h = next_h\n",
    "hidden_states = np.array(hidden_states).transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs231n.rnn_layers import rnn_forward, rnn_step_backward\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dh = np.random.randn(*out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dx, dprev_h, dWx, dWh, db = 0, 0, 0, 0, 0\n",
    "dWx_tot, dWh_tot, db_tot = 0, 0, 0\n",
    "dx_tot, dprev_h_tot = [None] * T, list()\n",
    "\n",
    "for t_i in reversed(range(T)):\n",
    "    dnext_h = dh[:,t_i,:] + dprev_h\n",
    "    dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache[t_i])\n",
    "\n",
    "    dx_tot[t_i] = dx\n",
    "    dprev_h_tot.append(dprev_h)\n",
    "    dWx_tot += dWx\n",
    "    dWh_tot += dWh\n",
    "    db_tot += db\n",
    "\n",
    "\n",
    "dx_tot = np.array(dx_tot).transpose(1,0,2)\n",
    "#dprev_h_tot = np.array(dprev_h_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs231n.rnn_layers import word_embedding_forward\n",
    "np.random.seed(231)\n",
    "\n",
    "N, T, V, D = 2, 3, 5, 6\n",
    "x = np.random.randint(V, size=(N, T))\n",
    "W = np.random.randn(V, D)\n",
    "\n",
    "out, cache = word_embedding_forward(x, W)\n",
    "dout = np.random.randn(*out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, out.shape, dout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, W.shape, out.shape, dout.shape, sep=\" \")\n",
    "print(\"\\n\")\n",
    "print(x, W, out, dout, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ix = [i for i in range(D)]\n",
    "print(np.add.at(W,[x.flatten(), d_ix],dout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x, W, out, dout, sep=\"\\n\\n\")\n",
    "\n",
    "(2, 3) (5, 6) (2, 3, 6) (2, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = word_embedding_backward(dout, cache)\n",
    "\n",
    "f = lambda W: word_embedding_forward(x, W)[0]\n",
    "dW_num = eval_numerical_gradient_array(f, W, dout)\n",
    "\n",
    "print('dW error: ', rel_error(dW, dW_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dWx_tot, dWh_tot, db_tot = [0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([2]*10).reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(a,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_vector = np.zeros((10,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d,e, f = np.array_split(activation_vector, 4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(5, 3, 224, 224)\n",
    "a = np.abs(a)\n",
    "b = np.max(a, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fooling images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "torch.Tensor([5])\n",
    "target_vector[0,5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(56)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([1, 2, 3])\n",
    "c = ((a + b)**2).sum()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.randint(10,(2,3,4,4))\n",
    "N,C,H,W = features.size()\n",
    "features_rs = features.view(N,C,-1)\n",
    "gram = torch.zeros((N,C,C))\n",
    "for n in range(N):\n",
    "    for i in range(C):\n",
    "        for j in range(C):\n",
    "            gram[n,i,j] = torch.dot(features_rs[n,j,:], features_rs[n,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[553., 469., 318.],\n",
       "         [469., 682., 291.],\n",
       "         [318., 291., 369.]],\n",
       "\n",
       "        [[476., 341., 381.],\n",
       "         [341., 434., 302.],\n",
       "         [381., 302., 541.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
